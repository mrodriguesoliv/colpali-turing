{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oorxJndfc-eT"
   },
   "source": [
    "---\n",
    "title: \"Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset\"\n",
    "description: \"Learn how to generate custom ColPali dataset using an open VLM for multimodal retrieval model training and fine-tuning.\"\n",
    "author: \"Daniel van Strien\"\n",
    "date: \"2024-09-23\"\n",
    "image: cJkPlaOVIju5yR7TaO0fD.png\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SL_819ac-ea"
   },
   "source": [
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davanstrien/blog/blob/main/posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5Jjm0vmc-ed"
   },
   "source": [
    "**updates**\n",
    "\n",
    "- You can find a follow up blog post on [using ColPali with Qdrant](https://danielvanstrien.xyz/posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html) which covers how to use the fine-tuned ColPali model with the Qdrant vector database to implement a multimodal retrieval pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJvOHEDBc-en"
   },
   "source": [
    "## Introduction to ColPali\n",
    "\n",
    "tl;dr this blog post covers how to generate a dataset for training/fine-tuning ColPali models an open VLM to generate queries. You can find the dataset produced by this approach [here](https://huggingface.co/datasets/davanstrien/ufo-ColPali).\n",
    "\n",
    "ColPali is a new multimodal approach to retrieval which aims to replace existing document retrievers which often rely on an OCR step with an end-to-end multimodal approach. This approach also aims to take into account the visual content and layout of the documents, in addition to the textual content. Looking at an example of a document:\n",
    "\n",
    "![](https://github.com/davanstrien/blog/blob/main/posts/post-with-code/colpali/doc.png?raw=1)\n",
    "\n",
    "we could rely only on the text, but the page also has a table which could be relevant for retrieving the most useful document either for direct use or in a RAG pipeline. In many documents we will find that pages don't just contain images but also other rich sources of visual information that could be relevant for retrieval.  \n",
    "\n",
    "## How CoPali works?\n",
    "\n",
    "ColPali is a document retrieval model that leverages a Vision Language Model (VLM) to understand and retrieve documents based on their visual content. The key steps are:\n",
    "\n",
    "- Document Encoding: ColPali takes document pages as images and processes them through a VLM (specifically, PaliGemma-3B) to generate embeddings for each image patch.\n",
    "- Query Encoding: User queries are encoded using the same VLM's text processing capabilities.\n",
    "- Late Interaction: Instead of comparing a single vector per document, ColPali uses a \"late interaction\" mechanism that compares each query token embedding to all document patch embeddings.\n",
    "\n",
    "Scoring: The relevance score between a query and a document is computed by summing the maximum similarities between each query token and all document patches.\n",
    "\n",
    "This approach allows ColPali to understand both textual and visual elements in documents, enabling more comprehensive and accurate retrieval compared to traditional text-only methods. It eliminates the need for complex document parsing pipelines, as it works directly with document images.\n",
    "\n",
    "Whilst, late interaction methods have some additional computational costs, for many documents with rich visual content there is also a high potential processing cost in the document parsing pipeline --- and this pipeline can also be rather brittle.\n",
    "\n",
    "### The training data for ColPali\n",
    "\n",
    "Let's take a look at what the training data looks like for training ColPali.\n",
    "\n",
    "\n",
    "<iframe\n",
    "  src=\"https://huggingface.co/datasets/vidore/colpali_train_set/embed/viewer/default/train\"\n",
    "  frameborder=\"0\"\n",
    "  width=\"100%\"\n",
    "  height=\"560px\"\n",
    "></iframe>\n",
    "\n",
    "\n",
    "You'll see that each row contains a bunch of metadata about the source of the document and other information but the key parts for the actual training of the model are the image and the queries pairs. When training ColPali we want a dataset of images with queries that relate to the image. This will allow the model to learn how queries are related to the images. To help the model learn it can also be helpful to have negative examples.\n",
    "\n",
    "Let's take a closer look at a few examples from the data before we jump into generating our own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3NBu9Mqc-eo"
   },
   "source": [
    "For this notebook I'm using `uv` to manage Python installs because I find it to be a lot quicker but you can use `pip` if you prefer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0t-7Y4Jc-ep",
    "outputId": "cfacb29a-fec8-4f42-825a-bee20a3e7b64"
   },
   "outputs": [],
   "source": [
    "#%pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67MC1w4PjTwf",
    "outputId": "17fc22ef-46e9-4008-d83f-02dfba8d3051"
   },
   "outputs": [],
   "source": [
    "#!uv pip install accelerate qwen-vl-utils torchvision torch datasets huggingface_hub[hf_transfer] polars --system\n",
    "#!uv pip install git+https://github.com/huggingface/transformers.git  --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8RHVF4Kc-et",
    "outputId": "013b43c0-12b4-4937-c403-5b23f5ee0938"
   },
   "outputs": [],
   "source": [
    "# !uv pip install flash-attn --no-build-isolation --system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lutvrf2nc-et"
   },
   "source": [
    "We can take a look at a few examples from the data using Polars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNNSSo6Zc-eu"
   },
   "outputs": [],
   "source": [
    "#import polars as pl\n",
    "\n",
    "#splits = {\"train\": \"data/train-*.parquet\", \"test\": \"data/test-00000-of-00001.parquet\"}\n",
    "#df = pl.scan_parquet(\n",
    "#    \"hf://datasets/vidore/colpali_train_set/\" + splits[\"train\"],\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kdjeG-Qc-ev"
   },
   "source": [
    "Let's see what columns we have in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axouK5WEc-ev",
    "outputId": "762f1faf-d771-4913-a5f8-5c68c462a794"
   },
   "outputs": [],
   "source": [
    "#df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4q2YM4Ac-ev"
   },
   "source": [
    "Since we're shortly going to turn to how we can generate our own queries, let's take a look at a few examples from the data. We'll filter to focus on the `pdf` source, since these are the ones created by the authors of ColPali (the other sources are from existing datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwcgP_g2c-ew",
    "outputId": "7e59beb7-f107-43ca-a70f-48838d13893f"
   },
   "outputs": [],
   "source": [
    "#filtered_df = (\n",
    "#    df.filter(pl.col(\"source\").str.contains(\"pdf\")).select([\"query\"]).head(10).collect()\n",
    "#)\n",
    "#query_list = filtered_df[\"query\"].to_list()\n",
    "#query_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHp7ybYHc-ew"
   },
   "source": [
    "::: {.callout-tip}\n",
    "One thing you might notice about these queries is that many of them are more focused on \"questions\" about documents rather than traditional search queries. We'll shortly see the prompting approach used to generate these queries but we might already want to consider, depending on our use case, whether we want to generate more \"search\"-like queries or more \"question\"-like queries.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCeHZ-dRc-ew"
   },
   "source": [
    "## Creating queries from documents\n",
    "\n",
    "For the data using in ColPali, part of the dataset was sourced from existing document question answering datasets. Another component was generated using Claude 3.0 Sonnet with the following prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQWUHyPOc-ew"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an assistant specialized in Multimodal RAG tasks.\n",
    "\n",
    "The task is the following: given an image from a pdf page, you will have to generate questions that can be asked by a user to retrieve information from a large documentary corpus.\n",
    "\n",
    "The question should be relevant to the page, and should not be too specific or too general. The question should be about the subject of the page, and the answer needs to be found in the page.\n",
    "\n",
    "Remember that the question is asked by a user to get some information from a large documentary corpus that contains multimodal data. Generate a question that could be asked by a user without knowing the existence and the content of the corpus.\n",
    "\n",
    "Generate as well the answer to the question, which should be found in the page. And the format of the answer should be a list of words answering the question.\n",
    "\n",
    "Generate at most THREE pairs of questions and answers per page in a dictionary with the following format, answer ONLY this dictionary NOTHING ELSE:\n",
    "\n",
    "{\n",
    "    \"questions\": [\n",
    "        {\n",
    "            \"question\": \"XXXXXX\",\n",
    "            \"answer\": [\"YYYYYY\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"XXXXXX\",\n",
    "            \"answer\": [\"YYYYYY\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"XXXXXX\",\n",
    "            \"answer\": [\"YYYYYY\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "where XXXXXX is the question and ['YYYYYY'] is the corresponding list of answers that could be as long as needed.\n",
    "\n",
    "Note: If there are no questions to ask about the page, return an empty list. Focus on making relevant questions concerning the page.\n",
    "\n",
    "Here is the page:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg7HgUBic-ex"
   },
   "source": [
    "\n",
    "As you can see this prompt is focused on generating questions that are relevant to the page and that could be asked by a user and answered by the document. One thing I noticed from the queries generated with this prompt is that I think some of the generated queries strayed a bit from the prompts request to not assume the user knew the content of the document. Some of the questions were quite specific and it seemed like they were tailored to the particular page they were generated from.\n",
    "\n",
    "Whilst out of the box performance of ColPali is likely to be good for many domains and use cases it it likely that fine tuning ColPali on domain specific data will lead to improved performance. We'll now turn to how we can generate our own queries for ColPali."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9X4eymnmc-ex"
   },
   "source": [
    "## UFO ColPali: creating a domain specific dataset\n",
    "\n",
    "Let's now turn to how we could approach generating our own query image pairs for training — or more likely fine tuning — a ColPali model on domain specific data.\n",
    "\n",
    "To make the example slightly more interesting we'll stray away from using an existing document dataset and use a UFO dateset which has been sourced from an Internet Archive [Collection of UFO newsletters](https://archive.org/details/ufonewsletters). This dataset was created from a sample of PDFs from this collection which were then split into single page images using the [pdf-to-page-images-dataset](https://huggingface.co/spaces/Dataset-Creation-Tools/pdf-to-page-images-dataset) Hugging Face Space. If you have a collection of PDFs related to your domain you could also use this Space to quickly create a dataset for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHTAozIfjs-k"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mrodriguesoliv/colpali-turing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMRjhEv6c-ey"
   },
   "source": [
    "If you are running on a machine with quite a fast connection, the following environment variable may increase the speed of the model and dataset downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfQOgg3Uc-ey"
   },
   "outputs": [],
   "source": [
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWJUEunVc-ey"
   },
   "source": [
    "Let's start by loading the UFO dataset and taking a look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "53bf7962827a4f6988385597615e47ee"
     ]
    },
    "id": "HXjwxaZkc-ey",
    "outputId": "f59243f5-dd77-4b63-8eee-adeafc8521e0"
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"davanstrien/ufo\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF8DiA_Ic-ez"
   },
   "source": [
    "Let's see what a row looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPOfgukic-ez",
    "outputId": "fe0b318a-bbf7-4b1d-b2a1-3372c046724e"
   },
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxYYl1Jvc-ez"
   },
   "source": [
    "and look at an example document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Swe3fYVyc-ez",
    "outputId": "dc3fda45-4294-414b-a199-a8a625f6ef92"
   },
   "outputs": [],
   "source": [
    "ds[3][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wXyAx4Uc-ez"
   },
   "source": [
    "We can see that the dataset currently just contains images which map to a single page of a document. We can also see from the example document that the document does contain some visual elements which could be relevant for retrieval.\n",
    "\n",
    "What we need to train or fine tune ColPali is at least one query for each of our the document images in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htP7YG_fc-e0"
   },
   "source": [
    "## Using Qwen2-VL to generate queries\n",
    "\n",
    "Whilst the original ColPali paper used the Claude 3.0 Sonnet model to generate queries for the UFO dataset, we'll use the Qwen2-VL model to generate queries for our UFO dataset. Specifcally we'll use the 7B variant of the model ([Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct)). This is an open VLLM (Apache 2.0 licensed) Vision Language Model which has shown strong performance on a variety of vision and language tasks. Whilst the model won't run on a standard T4 Google Colab instance we can run the model either on a L4 or an A100 GPU on Google Colab or you can use the Hugging Face Jupyter Spaces [template](https://huggingface.co/spaces/SpacesExamples/jupyterlab?duplicate=true) to run the model on an L40s as I did.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwg-T2-6c-e0"
   },
   "source": [
    "To start let's load the model and get a sense of how we can use it. We'll do this through the Transformers library. It's also possible to run the model using the vLLM library but since we're only focusing on generating a relatively small number of queries this extra set up probably isn't worth it in this case (I'd be interested to hear in the comments if you try it out and how it goes).\n",
    "\n",
    "To start we'll load the model. We'll use `flash_attention_2` which should help a bit with the performance and memory usage of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "e46643d6608f466f9137cb8ad76cbc63"
     ]
    },
    "id": "76Ug_08Sc-e0",
    "outputId": "feaaf596-fba1-4650-84e5-5f7387fdb315"
   },
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qP73NIp7c-e0"
   },
   "source": [
    "We next define the processor. This is the component that will help us prepare the inputs for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlBf0t5pc-e1"
   },
   "outputs": [],
   "source": [
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPBo0e58c-e1"
   },
   "source": [
    "As with many more recent models, we can use a chat template to help us prepare the inputs for the model. Here is the example from the Qwen2-VL-7B-Instruct model card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QL1vlbKGc-e1"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fY-0gVzsc-e2"
   },
   "source": [
    "We can now pass this to the processor to get the inputs ready for the model. You'll see here we first pass in the messages to the `apply_chat_template` method of the processor and then use the `process_vision_info` helper function which comes from the `qwen_vl_utils` library to prepare images and videos (which are not relevant here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3R57sMLcc-e2"
   },
   "outputs": [],
   "source": [
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZ2JLZfvc-e2"
   },
   "source": [
    "If we take a look at the `text` we can see that the processor has applied a chat template to the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYzaG4yrc-e8"
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gVpNo08c-e8"
   },
   "source": [
    "We now pass the text, image and video inputs (in this case `None`) to the processor and prepare the inputs for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RNEtyPWc-e9"
   },
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVL-6yBQc-e9"
   },
   "source": [
    "we can see what this input looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsIs3-Djc-e9",
    "outputId": "0c443dee-f068-4d83-e204-a2a88a4aa7e6"
   },
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xfbk1RWIc-e9"
   },
   "source": [
    "Now the inputs are ready to pass to the model. BTW, all of this inference code is copied straight from the Qwen2-VL-7B-Instruct model card on Hugging Face. There are a few things we might want to tweak but the basic examples are pretty much all we need for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXRNMVuHc-e9",
    "outputId": "553d6e8c-6ad9-4a01-c700-523f785c49ae"
   },
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=200)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki7PSqaUc-e-"
   },
   "source": [
    "## Building Colpali queries\n",
    "\n",
    "We now have a sense of how to generate responses using both a text and image input using the Qwen2-VL-7B-Instruct model. We'll now use this model to generate queries for our UFO dataset. To start, let's see how the prompt from the paper looks. The only thing we modified from the original prompt was to ask for JSON output rather than dictionaries since this model seemed to work better with this approach in my (somewhat limited) testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MANMLKYkc-e-"
   },
   "source": [
    "```python\n",
    "prompt = \"\"\"\n",
    "You are an assistant specialized in Multimodal RAG tasks.\n",
    "\n",
    "The task is the following: given an image from a pdf page, you will have to generate questions that can be asked by a user to retrieve information from a large documentary corpus.\n",
    "\n",
    "The question should be relevant to the page, and should not be too specific or too general. The question should be about the subject of the page, and the answer needs to be found in the page.\n",
    "\n",
    "Remember that the question is asked by a user to get some information from a large documentary corpus that contains multimodal data. Generate a question that could be asked by a user without knowing the existence and the content of the corpus.\n",
    "\n",
    "Generate as well the answer to the question, which should be found in the page. And the format of the answer should be a list of words answering the question.\n",
    "\n",
    "Generate at most THREE pairs of questions and answers per page as JSON with the following format, answer ONLY using JSON, NOTHING ELSE:\n",
    "\n",
    "{\n",
    "    \"questions\": [\n",
    "        {\n",
    "            \"question\": \"XXXXXX\",\n",
    "            \"answer\": [\"YYYYYY\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"XXXXXX\",\n",
    "            \"answer\": [\"YYYYYY\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"XXXXXX\",\n",
    "            \"answer\": [\"YYYYYY\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "where XXXXXX is the question and ['YYYYYY'] is the corresponding list of answers that could be as long as needed.\n",
    "\n",
    "Note: If there are no questions to ask about the page, return an empty list. Focus on making relevant questions concerning the page.\n",
    "\n",
    "Here is the page:\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRwURyiZc-e-"
   },
   "source": [
    "We'll copy and paste all of the previous code to generate a response from the model for an example from our UFO dataset. We'll wrap this in a function once we've got a better sense of what we're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qa6xOdbEmPWt"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": ds[0][\"image\"],\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wmz9Anhc-e-"
   },
   "outputs": [],
   "source": [
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p89DGpvIc-e_",
    "outputId": "336e64db-9caa-464a-db38-8a68158eb0e0"
   },
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=200)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRCl9i0pc-e_"
   },
   "source": [
    "You'll see we get some responses like this `\"What is the main event mentioned in the page?\"` which is a bit too specific and tailored to the particular page. There are a few reasons this might be happening but the first thing we should play around with is changing the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6JuQ7Umc-e_"
   },
   "source": [
    "### Validating the responses\n",
    "\n",
    "One of the big challenges you can have in generating synthetic data at scale is ensuring that you get valid responses that you can use in downstream tasks for training without having to do a lot of manual verification. Let's see if we can load the response as valid JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dtw9Ooxc-fA"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LrepP_pc-fA",
    "outputId": "4e6153d5-d886-4c7a-eb20-5221e0366304"
   },
   "outputs": [],
   "source": [
    "json.loads(output_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2A5i5dSc-fA",
    "outputId": "9b8f1b1a-0c31-4f2d-a906-4e6634b01420"
   },
   "outputs": [],
   "source": [
    "print(output_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rVZNdG8c-fB"
   },
   "source": [
    "Having a valid JSON is a good start but in many synthetic data generation tasks, people are increasingly using Pydantic to ensure outputs are valid in other ways.\n",
    "Let's take a look at a rewritten prompt I created for generating queries. In this prompt we ask the VLLM model to generate 3 different types of retrieval queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KopR-JbMc-fB"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an AI assistant specialized in document retrieval tasks. Given an image of a document page, your task is to generate retrieval queries that someone might use to find this document in a large corpus.\n",
    "\n",
    "Please generate 3 different types of retrieval queries:\n",
    "\n",
    "1. A broad topical query: This should cover the main subject of the document.\n",
    "2. A specific detail query: This should focus on a particular fact, figure, or point made in the document.\n",
    "3. A visual element query: This should reference a chart, graph, image, or other visual component in the document, if present.\n",
    "\n",
    "Important guidelines:\n",
    "- Ensure the queries are relevant for retrieval tasks, not just describing the page content.\n",
    "- Frame the queries as if someone is searching for this document, not asking questions about its content.\n",
    "- Make the queries diverse and representative of different search strategies.\n",
    "\n",
    "For each query, also provide a brief explanation of why this query would be effective in retrieving this document.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"broad_topical_query\": \"Your query here\",\n",
    "  \"broad_topical_explanation\": \"Brief explanation\",\n",
    "  \"specific_detail_query\": \"Your query here\",\n",
    "  \"specific_detail_explanation\": \"Brief explanation\",\n",
    "  \"visual_element_query\": \"Your query here\",\n",
    "  \"visual_element_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "If there are no relevant visual elements, replace the third query with another specific detail query.\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4h8TKeuic-fB"
   },
   "source": [
    "We ask the model for JSON output, we can represent this using a simple Pydantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTZlIMLUc-fB"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class GeneralRetrievalQuery(BaseModel):\n",
    "    broad_topical_query: str\n",
    "    broad_topical_explanation: str\n",
    "    specific_detail_query: str\n",
    "    specific_detail_explanation: str\n",
    "    visual_element_query: str\n",
    "    visual_element_explanation: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEPNrjssc-fD"
   },
   "source": [
    "We could add additional constraints to our Pydantic model for example we could set a minimum and maximum length for the queries and answers. We'll get back to this at the end of the post but for now we can make a start with this simpler approach.\n",
    "\n",
    "We'll now wrap this in a function to generate a response from the model using our Pydantic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM1gUdVcc-fD"
   },
   "source": [
    "## An update retrieval focused prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dR8whOB8c-fD",
    "outputId": "dcbbfdbb-e2c1-4f2e-89e4-15f77f7b41fc"
   },
   "outputs": [],
   "source": [
    "def get_retrieval_prompt(prompt_name: str) -> Tuple[str, GeneralRetrievalQuery]:\n",
    "    if prompt_name != \"general\":\n",
    "        raise ValueError(\"Only 'general' prompt is available in this version\")\n",
    "\n",
    "    prompt = \"\"\"You are an AI assistant specialized in document retrieval tasks. Given an image of a document page, your task is to generate retrieval queries that someone might use to find this document in a large corpus.\n",
    "\n",
    "Please generate 3 different types of retrieval queries:\n",
    "\n",
    "1. A broad topical query: This should cover the main subject of the document.\n",
    "2. A specific detail query: This should focus on a particular fact, figure, or point made in the document.\n",
    "3. A visual element query: This should reference a chart, graph, image, or other visual component in the document, if present.\n",
    "\n",
    "Important guidelines:\n",
    "- Ensure the queries are relevant for retrieval tasks, not just describing the page content.\n",
    "- Frame the queries as if someone is searching for this document, not asking questions about its content.\n",
    "- Make the queries diverse and representative of different search strategies.\n",
    "\n",
    "For each query, also provide a brief explanation of why this query would be effective in retrieving this document.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"broad_topical_query\": \"Your query here\",\n",
    "  \"broad_topical_explanation\": \"Brief explanation\",\n",
    "  \"specific_detail_query\": \"Your query here\",\n",
    "  \"specific_detail_explanation\": \"Brief explanation\",\n",
    "  \"visual_element_query\": \"Your query here\",\n",
    "  \"visual_element_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "If there are no relevant visual elements, replace the third query with another specific detail query.\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\"\"\"\n",
    "\n",
    "    return prompt, GeneralRetrievalQuery\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "prompt_name = \"general\"\n",
    "prompt, pydantic_model = get_retrieval_prompt(prompt_name)\n",
    "print(f\"Prompt for '{prompt_name}':\")\n",
    "print(prompt)\n",
    "print(f\"\\nPydantic model for this prompt: {pydantic_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXcHLJJGc-fE"
   },
   "source": [
    "We'll now also wrap this in a function to generate a response from the model using our Pydantic model. We could probably do a bit more refactoring here but this will do for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-Jq381dc-fE"
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt, image):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": image,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=200)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "    )\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnK0WOUOc-fE"
   },
   "source": [
    "Let's now generate a response from the model for an example image from our UFO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZNmkoLXc-fE",
    "outputId": "f2dd632c-0f15-483f-8548-d6f63dcf6331"
   },
   "outputs": [],
   "source": [
    "generate_response(prompt, ds[2][\"image\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUjH9dDmc-fF"
   },
   "source": [
    "We can see the model is doing a reasonable job of generating queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhqhw1Fmc-fF"
   },
   "source": [
    "```\n",
    "['{\\n  \"broad_topical_query\": \"Document discussing the possibility of a cameraman going on the record in the future\",\\n  \"broad_topical_explanation\": \"This query focuses on the main topic of the document, which is the discussion about the cameraman\\'s potential to go on the record.\",\\n  \"specific_detail_query\": \"Document mentioning Ray Santilli and his attempts to persuade the cameraman\",\\n  \"specific_detail_explanation\": \"This query targets a specific detail in the document, which is Ray Santilli\\'s efforts to contact the cameraman.\",\\n  \"visual_element_query\": \"Document containing images of a damaged leg and an alien\\'s foot\",\\n  \"visual_element_explanation\": \"This query refers to the visual elements present in the document, which are images of a damaged leg and an alien\\'s foot.\"\\n}']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dgv9ntohc-fF"
   },
   "source": [
    "```{.tip}\n",
    "One thing I have found in my experiments with generating synthetic data is that adding a request for an \"explanation\" from the model sometimes seems to help improve the quality of the generated data. I assume this is already noted somewhere in the literature (if not I'll call this `explain then generate`!). This seems to particularly helpful when generating more complex queries. Having the explanation can also give you a sense of how the model \"understands\" the task. This obviously comes with the donwside that it takes longer to generate the data and more tokens are required but it often seems worth trying.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQT6XbXic-fF"
   },
   "source": [
    "## Generating the full dataset\n",
    "\n",
    "As we play with the prompts and refine the queries we will often iterate quite quickly on a few examples. Once we're reasonably confident in the queries we can generate a larger dataset to see how well our prompts work across a larger set of examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTs6QriFc-fG",
    "outputId": "404633d9-813c-4b79-d31b-80789b100629"
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_jbxi_xc-fG"
   },
   "outputs": [],
   "source": [
    "sample = ds.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsBZ2Veoc-fG"
   },
   "source": [
    "To generate our full dataset we just wrap our previous code in a loop and run it for all the examples in our dataset. We add a very broad exception handler to catch any errors and continue with the next example. This is obviously not production code but it's good enough to get started with. If we scale to a much bigger dataset we might want to add some more robust error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "627ce7504864425cbbc6f2282dbd5132"
     ]
    },
    "id": "J2SLKIxMc-fG",
    "outputId": "5a10ced8-123e-4c30-99a6-c367d957e78a"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "responses = []\n",
    "for row in tqdm(sample):\n",
    "    try:\n",
    "        resp = generate_response(prompt, row[\"image\"])\n",
    "        responses.append(resp)\n",
    "    except Exception as e:\n",
    "        responses.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzg9DDsHc-fH",
    "outputId": "079cbd55-b12d-4126-cf4e-ab507eb4a99d"
   },
   "outputs": [],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kAgZ3pQc-fI"
   },
   "source": [
    "We can see how many errors we have in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CJwV6xpc-fJ",
    "outputId": "6f8dc588-30d9-43e1-d72e-c0156920be7e"
   },
   "outputs": [],
   "source": [
    "len([r for r in responses if r is None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRRqnrd6c-fJ"
   },
   "source": [
    "No bad generations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfwGGFKOc-fJ"
   },
   "source": [
    "We can also look at the first response to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGyJNEzJc-fJ",
    "outputId": "94289ede-6435-4a67-a418-671c02333ae1"
   },
   "outputs": [],
   "source": [
    "responses[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e8uanobc-fK"
   },
   "source": [
    "Let's see if we can parse this into a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3_T8hF1c-fK",
    "outputId": "ccbb3376-adb2-49c1-82d4-2c82b186fb16"
   },
   "outputs": [],
   "source": [
    "json.loads(responses[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwFob-3Wc-fK"
   },
   "source": [
    "first example seems to work, now let's add this to our dataset and see how many we can parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kHV-M5Dc-fK"
   },
   "outputs": [],
   "source": [
    "sample = sample.add_column(\"raw_queries\", responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_rSCZLrc-fL",
    "outputId": "9fb7c65f-2c09-4371-8696-00f756ae1d30"
   },
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXpkUeflc-fL"
   },
   "source": [
    "To deal with bad generations we'll create just fill out these column with `None` values. We can grab all the required keys from the valid first response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTYKu0_Ic-fL",
    "outputId": "eecd9adc-6a30-42c7-863f-4f967511d99d"
   },
   "outputs": [],
   "source": [
    "keys = list(json.loads(responses[0][0]).keys())\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z_pZxALc-fL"
   },
   "source": [
    "and do something like this to fill out this row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tmzAF3vc-fL",
    "outputId": "aee027c0-1d40-4c03-87a5-786220d4cf3b"
   },
   "outputs": [],
   "source": [
    "{k: None for k in keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzCu7YxBc-fM"
   },
   "source": [
    "We create a function to extract the data from the raw queries and parse them into a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vizuf9z9c-fM"
   },
   "outputs": [],
   "source": [
    "def extract_data(row):\n",
    "    try:\n",
    "        data = json.loads(row[\"raw_queries\"][0])\n",
    "        data[\"parsed_into_json\"] = True\n",
    "        return data\n",
    "    except Exception:\n",
    "        data = {k: None for k in keys}\n",
    "        data[\"parsed_into_json\"] = False\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ3Utnv6c-fM"
   },
   "source": [
    "We can now use the `map` method to apply this function to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P--zeFXMc-fM"
   },
   "outputs": [],
   "source": [
    "parsed_ds = sample.map(extract_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XP0L7S-Jc-fN"
   },
   "source": [
    "We can then see how many we've successfully parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxronXRyc-fN",
    "outputId": "a591e8a8-46e3-40b1-de61-946b9e530f96"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(parsed_ds[\"parsed_into_json\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lmb3H4Ltc-fN"
   },
   "source": [
    "So in this case 5% of the responses were not parseable. This isn't too bad and we might be able to live with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRH_4d-Pc-fN"
   },
   "source": [
    "## Pushing to the Hub\n",
    "\n",
    "We can now push this dataset to the Hugging Face Hub. This will also allow us to view the dataset in the Dataset Viewer. This can often be a very nice way of quickly checking through examples in a dataset to see how the quality looks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gM_xmNVc-fN"
   },
   "source": [
    "If you are not authenticated you can use the `login` function to authenticate with the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAtVQj9Gc-fN"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "bd4de41610614d4b83669c1cb8064e02"
     ]
    },
    "id": "lXzeiILvc-fO",
    "outputId": "5f395b15-d3e3-447e-b83f-918a0dc9a7af"
   },
   "outputs": [],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6fedd93a30f34aebb0207d2f3ca8c52c",
      "c4d1a1f443fe4b17976f7590434312d4",
      "b08b902c59d447798473df4d5e1eeec1"
     ]
    },
    "id": "9YkkNZmXc-fO",
    "outputId": "e6e908fb-aa4d-4e46-ea9e-ca414dd5020e"
   },
   "outputs": [],
   "source": [
    "parsed_ds.push_to_hub(\"davanstrien/ufo-ColPali\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBsRI3Jzc-fO"
   },
   "source": [
    "Here is what the dataset looks like in the Hugging Face Hub. You'll see there are actually more than a 100 examples since I did a larger generation of data since doing the first batch of a 100. You can be your own judge but my sense it that the queries are looking pretty good already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLUKDXjDc-fO"
   },
   "source": [
    "<iframe\n",
    "  src=\"https://huggingface.co/datasets/davanstrien/ufo-ColPali/embed/viewer/default/train\"\n",
    "  frameborder=\"0\"\n",
    "  width=\"100%\"\n",
    "  height=\"560px\"\n",
    "></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ss1jUdS-c-fP"
   },
   "source": [
    "## Conclusion, improvements and next steps\n",
    "\n",
    "There are a few improvements that we could make to this process.\n",
    "\n",
    "### Structured Generation\n",
    "\n",
    "One of the first things I think would be worth exploring further is using structured generation to improve the quality of the generated queries. This would allow us to properly use the Pydantic models to constrain the outputs. The `Outlines` library has functionality for doing this with for [VLMs](https://dottxt-ai.github.io/outlines/reference/models/transformers_vision/). Once I am more satisified with the quality of the queries I'll come back to this.\n",
    "\n",
    "### More diverse queries\n",
    "\n",
    "I focused on generating a single type of query for each example in the UFO dataset. I think for a dataset of this size it would be worth taking a more diverse set of generations. Below you can see an apendix with a few options for these kinds of queries.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "I am keen to test this approach with a few more domains and also work on the actual fine-tuning of ColPali models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nc-jTI2pc-fP"
   },
   "source": [
    "## Appendix more diverse queries\n",
    "\n",
    "Here are a few more examples of prompts I am experimenting with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRS3qftUc-fP"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "# Pydantic models for each prompt type\n",
    "\n",
    "\n",
    "class GeneralRetrievalQuery(BaseModel):\n",
    "    broad_topical_query: str\n",
    "    broad_topical_explanation: str\n",
    "    specific_detail_query: str\n",
    "    specific_detail_explanation: str\n",
    "    visual_element_query: str\n",
    "    visual_element_explanation: str\n",
    "\n",
    "\n",
    "class MultiDocumentComparisonQuery(BaseModel):\n",
    "    comparison_query: str\n",
    "    comparison_explanation: str\n",
    "    corroboration_contradiction_query: str\n",
    "    corroboration_contradiction_explanation: str\n",
    "\n",
    "\n",
    "class DomainSpecificQuery(BaseModel):\n",
    "    identified_domain: str\n",
    "    domain_specific_query: str\n",
    "    domain_specific_explanation: str\n",
    "    data_findings_query: str\n",
    "    data_findings_explanation: str\n",
    "    applications_implications_query: str\n",
    "    applications_implications_explanation: str\n",
    "\n",
    "\n",
    "class VisualElementFocusQuery(BaseModel):\n",
    "    similar_visual_element_query: str\n",
    "    similar_visual_element_explanation: str\n",
    "    text_visual_combination_query: str\n",
    "    text_visual_combination_explanation: str\n",
    "    visual_content_understanding_query: str\n",
    "    visual_content_understanding_explanation: str\n",
    "\n",
    "\n",
    "class TemporalMetadataQuery(BaseModel):\n",
    "    temporal_query: str\n",
    "    temporal_explanation: str\n",
    "    topic_metadata_combination_query: str\n",
    "    topic_metadata_combination_explanation: str\n",
    "    update_related_document_query: str\n",
    "    update_related_document_explanation: str\n",
    "\n",
    "\n",
    "class DifficultyAmbiguityQuery(BaseModel):\n",
    "    simple_query: str\n",
    "    simple_explanation: str\n",
    "    complex_query: str\n",
    "    complex_explanation: str\n",
    "    ambiguous_query: str\n",
    "    ambiguous_explanation: str\n",
    "\n",
    "\n",
    "class MultilingualMultimodalQuery(BaseModel):\n",
    "    multilingual_query: str\n",
    "    multilingual_explanation: str\n",
    "    multimodal_combination_query: str\n",
    "    multimodal_combination_explanation: str\n",
    "    text_visual_understanding_query: str\n",
    "    text_visual_understanding_explanation: str\n",
    "\n",
    "\n",
    "def get_retrieval_prompt(\n",
    "    prompt_name: str,\n",
    ") -> Tuple[\n",
    "    str,\n",
    "    Union[\n",
    "        GeneralRetrievalQuery,\n",
    "        MultiDocumentComparisonQuery,\n",
    "        DomainSpecificQuery,\n",
    "        VisualElementFocusQuery,\n",
    "        TemporalMetadataQuery,\n",
    "        DifficultyAmbiguityQuery,\n",
    "        MultilingualMultimodalQuery,\n",
    "    ],\n",
    "]:\n",
    "    prompts = {\n",
    "        \"general\": (\n",
    "            \"\"\"You are an AI assistant specialized in document retrieval tasks. Given an image of a document page, your task is to generate retrieval queries that someone might use to find this document in a large corpus.\n",
    "\n",
    "Please generate 3 different types of retrieval queries:\n",
    "\n",
    "1. A broad topical query: This should cover the main subject of the document.\n",
    "2. A specific detail query: This should focus on a particular fact, figure, or point made in the document.\n",
    "3. A visual element query: This should reference a chart, graph, image, or other visual component in the document, if present.\n",
    "\n",
    "Important guidelines:\n",
    "- Ensure the queries are relevant for retrieval tasks, not just describing the page content.\n",
    "- Frame the queries as if someone is searching for this document, not asking questions about its content.\n",
    "- Make the queries diverse and representative of different search strategies.\n",
    "\n",
    "For each query, also provide a brief explanation of why this query would be effective in retrieving this document.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"broad_topical_query\": \"Your query here\",\n",
    "  \"broad_topical_explanation\": \"Brief explanation\",\n",
    "  \"specific_detail_query\": \"Your query here\",\n",
    "  \"specific_detail_explanation\": \"Brief explanation\",\n",
    "  \"visual_element_query\": \"Your query here\",\n",
    "  \"visual_element_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "If there are no relevant visual elements, replace the third query with another specific detail query.\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n",
    "            GeneralRetrievalQuery,\n",
    "        ),\n",
    "        \"comparison\": (\n",
    "            \"\"\"Imagine this document page is part of a larger corpus. Your task is to generate retrieval queries that would require comparing this document with others in the corpus.\n",
    "\n",
    "Please generate 2 retrieval queries:\n",
    "\n",
    "1. A query comparing this document's topic with a related subject\n",
    "2. A query seeking documents that contradict or support the main point of this page\n",
    "\n",
    "For each query, provide a brief explanation of how it encourages document comparison and why it would be effective for retrieval.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"comparison_query\": \"Your query here\",\n",
    "  \"comparison_explanation\": \"Brief explanation\",\n",
    "  \"corroboration_contradiction_query\": \"Your query here\",\n",
    "  \"corroboration_contradiction_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n",
    "            MultiDocumentComparisonQuery,\n",
    "        ),\n",
    "        \"domain\": (\n",
    "            \"\"\"Your task is to create retrieval queries that a professional in the document's domain might use to find this document in a large corpus.\n",
    "\n",
    "First, identify the domain of the document (e.g., scientific, financial, legal, medical, technical).\n",
    "\n",
    "Then, generate 3 retrieval queries:\n",
    "\n",
    "1. A query using domain-specific terminology\n",
    "2. A query seeking specific data or findings presented in the document\n",
    "3. A query related to the document's potential applications or implications\n",
    "\n",
    "For each query, provide a brief explanation of its relevance to the domain and why it would be effective for retrieval.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"identified_domain\": \"Domain name\",\n",
    "  \"domain_specific_query\": \"Your query here\",\n",
    "  \"domain_specific_explanation\": \"Brief explanation\",\n",
    "  \"data_findings_query\": \"Your query here\",\n",
    "  \"data_findings_explanation\": \"Brief explanation\",\n",
    "  \"applications_implications_query\": \"Your query here\",\n",
    "  \"applications_implications_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n",
    "            DomainSpecificQuery,\n",
    "        ),\n",
    "        \"visual\": (\n",
    "            \"\"\"Your task is to generate retrieval queries focusing on the visual elements in this document page (charts, tables, images, diagrams).\n",
    "\n",
    "Please generate 3 retrieval queries:\n",
    "\n",
    "1. A query specifically asking for documents with similar visual elements\n",
    "2. A query combining textual and visual information\n",
    "3. A query that would require understanding the content of the visual element to retrieve this document\n",
    "\n",
    "For each query, provide a brief explanation of how it incorporates visual elements and why it would be effective for retrieval.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"similar_visual_element_query\": \"Your query here\",\n",
    "  \"similar_visual_element_explanation\": \"Brief explanation\",\n",
    "  \"text_visual_combination_query\": \"Your query here\",\n",
    "  \"text_visual_combination_explanation\": \"Brief explanation\",\n",
    "  \"visual_content_understanding_query\": \"Your query here\",\n",
    "  \"visual_content_understanding_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "If the document lacks significant visual elements, explain this and generate alternative queries focusing on the document's structure or layout.\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n",
    "            VisualElementFocusQuery,\n",
    "        ),\n",
    "        \"temporal\": (\n",
    "            \"\"\"Assuming this document is part of a large, diverse corpus, your task is to generate retrieval queries that incorporate metadata or temporal aspects.\n",
    "\n",
    "Please generate 3 retrieval queries:\n",
    "\n",
    "1. A query specifying a likely time frame for this document\n",
    "2. A query combining topical information with a metadata element (e.g., author, publication type)\n",
    "3. A query seeking updated or related documents on the same topic\n",
    "\n",
    "For each query, provide a brief explanation of how it uses temporal or metadata information and why it would be effective for retrieval.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"temporal_query\": \"Your query here\",\n",
    "  \"temporal_explanation\": \"Brief explanation\",\n",
    "  \"topic_metadata_combination_query\": \"Your query here\",\n",
    "  \"topic_metadata_combination_explanation\": \"Brief explanation\",\n",
    "  \"update_related_document_query\": \"Your query here\",\n",
    "  \"update_related_document_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n",
    "            TemporalMetadataQuery,\n",
    "        ),\n",
    "        \"difficulty\": (\n",
    "            \"\"\"Your task is to create retrieval queries for this document at different levels of complexity and ambiguity.\n",
    "\n",
    "Please generate 3 retrieval queries:\n",
    "\n",
    "1. A simple, straightforward query\n",
    "2. A complex query requiring understanding of multiple aspects of the document\n",
    "3. An ambiguous query that could retrieve this document among others\n",
    "\n",
    "For each query, provide a brief explanation of its complexity level or ambiguity and why it would be effective or challenging for retrieval.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"simple_query\": \"Your query here\",\n",
    "  \"simple_explanation\": \"Brief explanation\",\n",
    "  \"complex_query\": \"Your query here\",\n",
    "  \"complex_explanation\": \"Brief explanation\",\n",
    "  \"ambiguous_query\": \"Your query here\",\n",
    "  \"ambiguous_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n",
    "            DifficultyAmbiguityQuery,\n",
    "        ),\n",
    "        \"multilingual\": (\n",
    "            \"\"\"Your task is to generate retrieval queries considering potential multilingual and multi-modal aspects of the document.\n",
    "\n",
    "Please generate 3 retrieval queries:\n",
    "\n",
    "1. A query in a different language (if applicable) that would retrieve this document\n",
    "2. A query combining textual and non-textual elements\n",
    "3. A query that requires understanding both the text and visual elements to retrieve this document accurately\n",
    "\n",
    "For each query, provide a brief explanation of its multilingual or multi-modal nature and why it would be effective for retrieval.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"multilingual_query\": \"Your query here\",\n",
    "  \"multilingual_explanation\": \"Brief explanation\",\n",
    "  \"multimodal_combination_query\": \"Your query here\",\n",
    "  \"multimodal_combination_explanation\": \"Brief explanation\",\n",
    "  \"text_visual_understanding_query\": \"Your query here\",\n",
    "  \"text_visual_understanding_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "If the document is not suitable for multilingual queries, explain why and provide an alternative query.\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n",
    "            MultilingualMultimodalQuery,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    if prompt_name not in prompts:\n",
    "        raise ValueError(\n",
    "            f\"Invalid prompt name. Choose from: {', '.join(prompts.keys())}\"\n",
    "        )\n",
    "\n",
    "    return prompts[prompt_name]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "prompt_name = \"general\"  # You can change this to any of the available prompt names\n",
    "prompt, pydantic_model = get_retrieval_prompt(prompt_name)\n",
    "print(f\"Prompt for '{prompt_name}':\")\n",
    "print(prompt)\n",
    "print(f\"\\nPydantic model for this prompt: {pydantic_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScxbHb_qc-fQ"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
